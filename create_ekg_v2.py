# -*- coding: utf-8 -*-
"""primesoft_create_EKG_v2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SBYeU46V4XXrIrtliTDnlZ-PhPyhUXG8

#Connect to Google Drive
"""

# Connect to Google Drive
from google.colab import drive
drive.mount('/content/drive')

"""# Import Libraries"""

!pip install openai python-dotenv PyMuPDF pypdf pdfkit tiktoken networkx --upgrade -q

import os
import asyncio
import fitz
import openai
from dotenv import load_dotenv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import datetime
import json
import pdfkit
import shutil
import fitz  # PyMuPDF
import pdfkit
import networkx as nx

from itertools import islice
from uuid import uuid4
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
from openai import OpenAI
from pprint import pprint

from IPython.display import display, Markdown
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from pypdf import PdfReader
import numpy as np
from tqdm import tqdm

import time
import warnings
warnings.filterwarnings('ignore')

"""# Functions

## File operations: Convert files, extract text
"""

def get_extension(file_path):
    return os.path.splitext(file_path)[-1].lower()

def convert_html_to_pdf(input_path):
    output_path = input_path.replace(".html", ".pdf")
    try:
      pdfkit.from_file(input_path, output_path)
      # print(f'saved to {output_path}')
      return output_path
    except Exception as e:
      print(f"Error converting {input_path} to PDF: {e}")
      return None

def convert_excel_to_annotated_txt(file_path):
    if not os.path.exists(file_path): # Debug check
        print(f"Error: File not found at {file_path}")
        return None

    try:
      # print(f"Attempting to open Excel file: {file_path}") # Debug print
      excel_file = pd.ExcelFile(file_path)
      output_text = []
      output_text.append(f"This file was converted from an Excel workbook: {os.path.basename(file_path)}")
      output_text.append("It contains the following sheets with tabular data:\n")

      for sheet in excel_file.sheet_names:
          df = pd.read_excel(excel_file, sheet_name=sheet)
          output_text.append(f"--- Sheet: {sheet} ---")
          output_text.append(df.to_string(index=False))
          output_text.append("\n")
      annotated_path = file_path.replace(".xlsx", ".txt")
      with open(annotated_path, "w", encoding="utf-8") as f:
          f.write("\n".join(output_text))
          # print(f'saved to {annotated_path}')
      return annotated_path

    except Exception as e:
      print(f"Error converting {file_path} to text: {e}")
      return None


def convert_files(file_path):

  # for file_path in file_list:
      extension_ = '.' + os.path.basename(file_path).split('.')[-1]
      # print(file_path)
      if extension_ in SUPPORTED_DIRECT:
          return {'file path':file_path, 'converted_file':file_path}

      elif extension_ in CONVERTIBLE_TYPES:
          if extension_ == '.xlsx':
              new_path = convert_excel_to_annotated_txt(file_path)
              # print(f'saved as {new_path}')
              return {'file path':file_path, 'converted_file':new_path}

          elif extension_ == '.html':
              new_path = convert_html_to_pdf(file_path)
              # print(f'saved as {new_path}')
              return {'file path':file_path, 'converted_file':new_path}

      else:
          print(f"Unsupported file type: {extension_}/ {file_path}")
          return {'file path':file_path, 'converted_file':None}

# Function to extract all text from PDF
def extract_text_from_pdf(pdf_path):
    reader = PdfReader(pdf_path)
    text = ''
    for page in reader.pages:
        try:
            text += page.extract_text() + '\n'
        except Exception as e:
            print(f"Warning: Couldn't extract text from a page: {e}")
    return text

def extract_text_from_txt(txt_path):
    with open(txt_path, 'r', encoding='utf-8', errors='ignore') as f:
        return f.read()

def extract_text_from_csv(csv_path):
    lines = []
    with open(csv_path, 'r', encoding='utf-8', errors='ignore') as f:
        reader = csv.reader(f)
        for row in reader:
            lines.append(' '.join(row))
    return '\n'.join(lines)

def extract_text_from_md(md_path):
    with open(md_path, 'r', encoding='utf-8', errors='ignore') as f:
        return f.read()

def transfer_files_to_upload_folder(file_path, upload_directory):

  # for file_path in file_list:
    extension_ = get_extension(file_path)
    if get_extension(file_path) in SUPPORTED_DIRECT:
      destination_path = os.path.join(upload_directory, os.path.basename(file_path))

      # Check if the destination file already exists
      if os.path.exists(destination_path):
          print(f"Destination file '{destination_path}' already exists. Removing it.")
          # os.remove(destination_path) # Remove the existing file
      else:
          shutil.copy(file_path, upload_directory)

# traverse all folders and get all file_paths

def get_all_file_paths(directory):
    file_paths = []
    for root, dirs, files in os.walk(directory):
        for file in files:
            # Construct absolute file path
            absolute_path = os.path.abspath(os.path.join(root, file))
            file_paths.append(absolute_path)
    return file_paths

"""## Upload to openAI"""

def upload_file_to_openai(file_path):
    with open(file_path, "rb") as f:
      try:
        response = openai.files.create(file=f, purpose="assistants")
      except:
        print(f"Error uploading {file_path}")
        return None
    return response.id

# def upload_to_openai(source_directory, upload_directory):

#     # Add to or create metadata file
#     if os.path.exists(METADATA_INDEX_FILE):
#         metadata = pd.read_excel(METADATA_INDEX_FILE)
#     else:
#         metadata = pd.DataFrame(columns=["file_name", "file_id", "file_type", "file_category",  "uploaded_at"])

#     dir_list = os.listdir(source_directory)
#     print(dir_list)
#     file_category = {}

#     for directory in dir_list:
#         category = directory
#         directory = os.path.join(source_directory, directory)

#         for file in os.listdir(directory):
#           file_category[file] = category

#         convert_files(directory)
#         transfer_files_to_upload_folder(directory, upload_directory)

#     for file_name in os.listdir(upload_directory):
#         file_path = os.path.join(upload_directory, file_name)

#         if file_name in metadata['file_name'].values:
#             print(f" {file_name} + already uploaded")
#         else:

#             try:
#                 file_id = upload_file_to_openai(file_path)
#                 metadata_entry = {
#                     "file_name": os.path.basename(file_path),
#                     "file_id": file_id,
#                     "file_type": get_extension(file_path),
#                     "file_category": file_category[os.path.basename(file_path)],
#                     "uploaded_at": datetime.datetime.now().isoformat(),
#                 }
#                 print(metadata_entry)
#                 new_entry_df = pd.DataFrame(metadata_entry, index=[0])
#                 metadata = pd.concat([metadata, new_entry_df], ignore_index=True)

#             except ValueError as e:
#                 print("âŒ", str(e))
#                 return None

#     metadata.to_excel(METADATA_INDEX_FILE, index=False)
#     return metadata

"""# Parameter definitions"""

# Set the OPENAI_API_KEY environment variable
os.environ['OPENAI_API_KEY'] = ""
client = OpenAI(api_key = os.getenv("OPENAI_API_KEY"))

# These are the file types currently supported
SUPPORTED_DIRECT = ['.txt', '.pdf', '.csv', '.md']
CONVERTIBLE_TYPES = ['.xlsx', '.html']
no_of_features = 10

# EKG mapping - source path and vectore store mapping
# base_path = '/content/drive/My Drive/Wealth EKG'
base_path = '/content/drive/MyDrive/primesoft documents'
# attributes =  pd.read_excel(os.path.join(base_path, 'Metadata', 'attributes_list.xlsx'))

EKG_map = {
    'Primesquare':  {'vector_store':'vs_68ea1f9e59b8819193d3c092779bb47e', #primesquare
                     'source_path':'primesquare',
                    #  'attributes': attributes['Product_attributes'].dropna().to_list(),
                     },
}
METADATA_INDEX_FILE = os.path.join(base_path, 'Metadata', "file_metadata_index.xlsx")

# 1. Get the list of all files in the source folder
metadata_columns = ['file path', 'file_name', 'file_id',
                    'EKG', 'converted_file', 'conversion_status',
                    'attribute 1', 'attribute 2', 'attribute 3', 'attribute 4', 'attribute 5',
                    'attribute 6', 'attribute 7', 'attribute 8', 'attribute 9', 'attribute 10',
                    'customer', 'analysis_status', 'fs_upload_status', 'uploaded_at']

# 1. Read metadata file
if os.path.exists(METADATA_INDEX_FILE):
  metadata = pd.read_excel(METADATA_INDEX_FILE)
else:
  metadata = pd.DataFrame(columns = metadata_columns)
# metadata = pd.read_excel(os.path.join(base_path, 'file_attribute_list.xlsx'))
metadata = metadata.set_index('file path')

"""## If new files are uploaded, process conversion"""

all_new_files = pd.DataFrame()

for key, value in EKG_map.items():
    print(f'\n{key}:')
    source_path = os.path.join(base_path, value['source_path'])
    file_list = get_all_file_paths(source_path)
    file_list_df = pd.DataFrame(file_list, columns=['file path'])
    file_list_df['file name'] = file_list_df['file path'].apply(lambda x: os.path.basename(x))
    file_list_df['EKG'] = key
    file_list_df = file_list_df.set_index('file path')
    file_index = pd.DataFrame(index=file_list_df.index)
    print(f'total files: {len(file_list_df)}')
    metadata = pd.merge(metadata, file_index, left_index=True, right_index=True, how='outer')
    metadata.update(file_list_df)
    new_files = metadata[(metadata['EKG'] == key) & (metadata['conversion_status'].isna())]
    print(f'new files: {new_files.shape[0]}')

# Identify new files uploaded to the source folder
new_files_ = metadata[(metadata['conversion_status'].isna())].index.to_list()

if not new_files_:
  print('No new files to process')
else:
  file_paths = []

  #convert new file to supported format
  for file in tqdm(new_files_):
    file_paths.append(convert_files(file))
  file_paths_df = pd.DataFrame(file_paths)

  file_paths_df = file_paths_df.set_index('file path')
  metadata.update(file_paths_df)

for index,row in metadata.iterrows():
  if pd.isna(row['converted_file']):
    metadata.loc[index, 'conversion_status'] = 'conversion_failed'
    # Leave status as NaN if converted_file is None
  elif row['converted_file'] == index:
    metadata.loc[index, 'conversion_status'] = 'original'
  elif row['converted_file'] != index:
    metadata.loc[index, 'conversion_status'] = 'converted'

metadata.reset_index().to_excel(METADATA_INDEX_FILE, index=False)

os.listdir(base_path)

"""## Generat list of all files in the EKG"""

from itertools import islice
# file_list = get_all_file_paths(os.path.join(base_path, islice(
for key, values in islice(EKG_map.items(), 1):
  source_path = os.path.join(base_path, values['source_path'])
  file_list = get_all_file_paths(source_path)

file_list_df = pd.DataFrame(file_list, columns = ['full_path'])
file_list_df['filename'] = [ os.path.basename(file) for file in file_list_df['full_path']]

file_list_df

file_list_df['main_folder'] = [file.split(source_path)[1].split('/')[1] for file in file_list_df['full_path']]

for n in range(2,8):
  sub_folders = []

  for file in file_list_df['full_path']:
    try:
      foldername = file.split(source_path)[1].split('/')[n]
      if foldername != os.path.basename(file):
        sub_folders.append(file.split(source_path)[1].split('/')[n])
      else:
        sub_folders.append('NA')
    except:
      sub_folders.append('NA')
  column_name = f'sub_folder_{n-1}'
  file_list_df[column_name] = sub_folders

file_list_df.to_excel(os.path.join(base_path , 'file_list.xlsx'))

"""## Generate metadata"""

# metadata = pd.read_excel(METADATA_INDEX_FILE).set_index('converted_file')
# batch_size = 10
# files_to_analyse = metadata[(metadata['analysis_status'].isna()) &
#                             (metadata['conversion_status']!= 'conversion_failed')]

# print(f'extracting attributes:\n')
# # process files one by one if analysis was not done earlier
# counter = 0
# for index, row in tqdm(files_to_analyse.iterrows()):
#   if row['analysis_status'] == 'completed':
#     # print(f'skipping {index}')
#     continue
#   else:
#     try:
#       descriptions = EKG_map[row['EKG']]['attributes']
#       attributes = get_metadata(index, descriptions, no_of_features)
#       # print(f'{index}\n{attributes}')
#       attributes.update({'analysis_status':'completed'})
#       attributes = pd.DataFrame([attributes]).set_index('converted_file')
#       metadata.update(attributes)

#       counter += 1
#       if counter % batch_size == 0:
#         metadata.reset_index().to_excel(METADATA_INDEX_FILE, index=False)
#     except Exception as e:
#       print(f'Error processing {index}: {e}')
#       continue
# metadata.reset_index().to_excel(METADATA_INDEX_FILE, index=False)

"""## Metadata Extractiion workflow"""

class DocumentTagExtractor:
    """Extract tags from uploaded files BEFORE creating vector store for search"""

    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)

    def extract_tags_from_file_content(self, file_id: str, available_tags: List[str],
                                     document_name: str = "Document") -> Dict:
        """
        Extract tags by directly reading file content uploaded to openAI filestore
        """

        try:
            # Get file content directly
            file_content = self.client.files.content(file_id)
            file_text = file_content.read().decode('utf-8')

            # Extract key sections for analysis
            key_content = self.extract_business_sections(file_text)

            # Use direct completion API (not assistants) for tagging
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",  # Most cost-effective
                messages=[
                    {
                        "role": "system",
                        "content": """You are a business classification expert. Focus on the company's PRIMARY business activities, not industries they serve or mention.

CRITICAL RULES:
- A bank that serves oil companies is "Banking" NOT "Oil and Gas"
- A tech company serving banks is "Software" NOT "Banking"
- Focus on core revenue-generating activities
- Ignore client industries mentioned in passing"""
                    },
                    {
                        "role": "user",
                        "content": f"""Document: {document_name}

Key Content: {key_content[:4000]}

Available Tags: {', '.join(available_tags)}

Task: Select 3-5 tags that best describe this company's PRIMARY business. Respond in JSON format:

{{
    "primary_tags": ["tag1", "tag2", "tag3"],
    "confidence_scores": [95, 85, 75],
    "reasoning": "Brief explanation for top tag",
    "company_type": "One-line business description"
}}"""
                    }
                ],
                temperature=0.1,
                max_tokens=500
            )

            # Parse response
            response_text = response.choices[0].message.content
            result = self.parse_tagging_response(response_text, available_tags)

            # Add usage info for cost tracking
            result['usage'] = {
                'input_tokens': response.usage.prompt_tokens,
                'output_tokens': response.usage.completion_tokens,
                'total_tokens': response.usage.total_tokens,
                'estimated_cost': (response.usage.prompt_tokens * 0.00015 +
                                 response.usage.completion_tokens * 0.0006) / 1000
            }

            return result

        except Exception as e:
            print(f"Error processing file {file_id}: {e}")
            return {
                'primary_tags': [],
                'confidence_scores': [],
                'reasoning': f'Error: {str(e)}',
                'company_type': 'Unable to determine',
                'usage': {'total_tokens': 0, 'estimated_cost': 0}
            }

    def extract_business_sections(self, document_text: str, max_chars: int = 4000) -> str:
        """Extract the most relevant business description sections"""

        import re

        # Look for key business sections
        business_patterns = [
            r'(?:business\s+overview|about\s+(?:us|the\s+company)|our\s+business|company\s+profile).*?(?=\n\s*(?:[A-Z][^a-z]*\n|\d+\.|[A-Z]{2,}|\Z))',
            r'(?:executive\s+summary|management\s+discussion).*?(?=\n\s*(?:[A-Z][^a-z]*\n|\d+\.|[A-Z]{2,}|\Z))',
            r'(?:primary\s+activities|core\s+business|main\s+business|business\s+segments).*?(?=\n\s*(?:[A-Z][^a-z]*\n|\d+\.|[A-Z]{2,}|\Z))'
        ]

        extracted_sections = []
        doc_lower = document_text.lower()

        for pattern in business_patterns:
            matches = re.findall(pattern, doc_lower, re.IGNORECASE | re.DOTALL)
            for match in matches:
                if len(match.strip()) > 100:  # Only meaningful content
                    extracted_sections.append(match.strip())

        # If no specific sections found, use strategic sampling
        if not extracted_sections:
            doc_len = len(document_text)
            if doc_len > 6000:
                # Take first 2000 chars (usually contains business overview)
                # Take section around 1/3 mark (often contains business details)
                # Take a section from 2/3 mark (more business info)
                first_part = document_text[:2000]
                middle_part = document_text[doc_len//3 - 500:doc_len//3 + 1500]
                later_part = document_text[2*doc_len//3 - 500:2*doc_len//3 + 1000]
                extracted_sections = [first_part, middle_part, later_part]
            else:
                extracted_sections = [document_text]

        # Combine and limit
        combined = ' '.join(extracted_sections)
        return combined[:max_chars]

    def parse_tagging_response(self, response_text: str, available_tags: List[str]) -> Dict:
        """Parse JSON or free-form tagging response"""

        try:
            # Try to parse as JSON first
            if '{' in response_text and '}' in response_text:
                json_start = response_text.find('{')
                json_end = response_text.rfind('}') + 1
                json_text = response_text[json_start:json_end]
                result = json.loads(json_text)

                # Validate tags
                if 'primary_tags' in result:
                    validated_tags = []
                    for tag in result['primary_tags']:
                        if tag in available_tags:
                            validated_tags.append(tag)
                    result['primary_tags'] = validated_tags

                return result
        except:
            pass

        # Fallback: parse free-form response
        return self.parse_freeform_response(response_text, available_tags)

    def parse_freeform_response(self, response_text: str, available_tags: List[str]) -> Dict:
        """Parse free-form response for tags"""

        found_tags = []
        response_lower = response_text.lower()

        # Look for tags in response
        for tag in available_tags:
            if tag.lower() in response_lower:
                found_tags.append(tag)

        # Extract reasoning (look for explanations)
        reasoning = "Extracted from response text"
        if "because" in response_lower or "reason" in response_lower:
            sentences = response_text.split('.')
            for sentence in sentences:
                if "because" in sentence.lower() or "reason" in sentence.lower():
                    reasoning = sentence.strip()[:200]
                    break

        return {
            'primary_tags': found_tags[:5],
            'confidence_scores': [80] * len(found_tags[:5]),
            'reasoning': reasoning,
            'company_type': 'Parsed from free-form response'
        }

    def batch_extract_tags(self, file_mappings: Dict[str, str], available_tags: List[str]) -> Dict[str, Dict]:
        """
        Extract tags from multiple files for vector store preparation

        Args:
            file_mappings: {file_id: document_name}
            available_tags: List of possible tags

        Returns:
            {document_name: {tags, confidence, reasoning, etc.}}
        """

        results = {}
        total_cost = 0

        print(f"Extracting tags from {len(file_mappings)} files for vector store creation...")

        for i, (file_id, doc_name) in enumerate(file_mappings.items(), 1):
            print(f"\nProcessing {i}/{len(file_mappings)}: {doc_name}")

            try:
                result = self.extract_tags_from_file_content(file_id, available_tags, doc_name)
                results[doc_name] = result

                cost = result.get('usage', {}).get('estimated_cost', 0)
                total_cost += cost

                print(f"  Tags: {result.get('primary_tags', [])}")
                print(f"  Confidence: {result.get('confidence_scores', [])}")
                print(f"  Cost: ${cost:.4f}")

            except Exception as e:
                print(f"  Error: {e}")
                results[doc_name] = {
                    'primary_tags': [],
                    'error': str(e),
                    'usage': {'estimated_cost': 0}
                }

            # Rate limiting
            time.sleep(0.5)

        print(f"\nTag extraction complete. Total cost: ${total_cost:.4f}")
        return results

class TaggedVectorStoreCreator:
    """Create vector store with extracted tags as metadata"""

    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)

    def create_tagged_vector_store(self, file_tag_mappings: Dict[str, Dict],
                                 store_name: str = "Tagged Document Store") -> str:
        """
        Create vector store with tag metadata for better search

        Args:
            file_tag_mappings: {file_id: {primary_tags: [...], ...}}
            store_name: Name for the vector store

        Returns:
            vector_store_id
        """

        # Create vector store
        vector_store = self.client.beta.vector_stores.create(name=store_name)

        # Add files with metadata
        for file_id, tag_info in file_tag_mappings.items():
            # Create metadata with tags
            metadata = {
                'primary_tags': ','.join(tag_info.get('primary_tags', [])),
                'confidence_scores': ','.join(map(str, tag_info.get('confidence_scores', []))),
                'company_type': tag_info.get('company_type', 'Unknown'),
                'reasoning': tag_info.get('reasoning', '')[:100]  # Limit metadata size
            }

            # Add file to vector store
            self.client.beta.vector_stores.files.create(
                vector_store_id=vector_store.id,
                file_id=file_id,
                metadata=metadata
            )

            time.sleep(0.1)  # Rate limiting

        print(f"Created tagged vector store: {vector_store.id}")
        return vector_store.id

    def create_search_assistant(self, vector_store_id: str) -> str:
        """Create assistant for searching the tagged vector store"""

        assistant = self.client.beta.assistants.create(
            name="Tagged Document Search Assistant",
            instructions="""You are a document search assistant with access to tagged business documents.

Each document has been pre-tagged with its primary business categories. Use these tags to:
1. Find documents by business type (e.g., "show me all banking documents")
2. Compare similar companies (e.g., "compare banking vs fintech companies")
3. Provide contextual answers based on document tags
4. Filter searches by industry or business model

When responding, mention the relevant document tags to help users understand the context.""",
            model="gpt-4o",
            tools=[{"type": "file_search"}],
            tool_resources={
                "file_search": {
                    "vector_store_ids": [vector_store_id]
                }
            }
        )

        return assistant.id

# Complete workflow solution
class TagExtractionWorkflow:
    """Complete workflow: Extract tags â†’ Create tagged vector store â†’ Enable search"""

    def __init__(self, api_key: str):
        self.extractor = DocumentTagExtractor(api_key)
        self.store_creator = TaggedVectorStoreCreator(api_key)

    def complete_workflow(self, file_mappings: Dict[str, str],
                         available_tags: List[str]) -> Tuple[str, str, Dict]:
        """
        Complete workflow from files to tagged search system

        Returns:
            (vector_store_id, assistant_id, tag_results)
        """

        print("ðŸ·ï¸  STEP 1: Extracting tags from documents...")
        tag_results = self.extractor.batch_extract_tags(file_mappings, available_tags)

        print("\nðŸ“š STEP 2: Creating tagged vector store...")
        # Map file_ids to tag results
        file_tag_mappings = {}
        for doc_name, tag_info in tag_results.items():
            # Find file_id for this doc_name
            file_id = None
            for fid, dname in file_mappings.items():
                if dname == doc_name:
                    file_id = fid
                    break
            if file_id:
                file_tag_mappings[file_id] = tag_info

        vector_store_id = self.store_creator.create_tagged_vector_store(file_tag_mappings)

        print("\nðŸ” STEP 3: Creating search assistant...")
        assistant_id = self.store_creator.create_search_assistant(vector_store_id)

        print(f"\nâœ… WORKFLOW COMPLETE!")
        print(f"   Vector Store ID: {vector_store_id}")
        print(f"   Assistant ID: {assistant_id}")
        print(f"   Documents processed: {len(tag_results)}")

        return vector_store_id, assistant_id, tag_results

# Usage example
def example_workflow():
    """Example of complete tag-first workflow"""

    api_key = "your-openai-api-key"

    # Your file mappings (file_id -> document_name)
    file_mappings = {
        "file-abc123": "HDFC Bank Annual Report",
        "file-def456": "Reliance Industries Report",
        "file-ghi789": "TCS Annual Report"
        # Add your actual file IDs here
    }

    # Your available tags
    available_tags = [
        'Oil and Gas', 'Retail', 'Telecommunications', 'Banking',
        'Financial services', 'Energy', 'Software', 'Motor vehicles',
        'Cement', 'Chemicals', 'Pharmaceuticals', 'Reliance Group', 'Tata Group'
    ]

    # Run complete workflow
    workflow = TagExtractionWorkflow(api_key)
    vector_store_id, assistant_id, tag_results = workflow.complete_workflow(
        file_mappings, available_tags
    )

    # Now you can search with tags!
    # Example searches:
    # "Show me all banking companies"
    # "Compare oil and gas vs energy companies"
    # "What are the main business activities of software companies?"

    return vector_store_id, assistant_id, tag_results

if __name__ == "__main__":
    example_workflow()

"""# Execution

## Delete existing files from filestore, if needed
"""

# Step 1: List all files
fs_files = openai.files.list()
fs_file_names = [file_.filename for file_ in fs_files.data] #list of files already uploaded to openAI file store

# Step 2: Delete each file by its ID
for file in fs_files.data:
    file_id = file.id
    openai.files.delete(file_id)
    print(f"Deleted file: {file_id}\n{file.filename}")

"""## Print list of vector stores"""

# Get list of existing vector stores
vs_list = client.vector_stores.list()
print(f'{len(vs_list.data)} vector stores are active')
for vs in vs_list.data:
  print(f'id: {vs.id}, Name: {vs.name}, File count: {vs.file_counts.total}')

#vs_689b49252a4c8191a12a1528a475fbd8

"""## Delete vector store if needed"""

# # Delete vector stores, put vector store ids to delete
# vs_list_ = []

# for vs in vs_list_:
#   response = client.vector_stores.delete(vs)
#   print(f'deleted {vs}')

"""## Detach files from the vector store, if needed"""

# # Get list of vector stores
# vs_list = client.vector_stores.list()
# for n, vs in enumerate(vs_list.data):
#   print(vs.id, vs.name)
#   print(vs.file_counts)

# select vector store
vectorstore_id = 'vs_689b49252a4c8191a12a1528a475fbd8'

# see list of files
vs_files = client.vector_stores.files.list(vector_store_id=vectorstore_id)
# for vs_file in vs_files.data:
#     print(vs_file.attributes['original_filename'])

vectorstore_id = 'vs_689b49252a4c8191a12a1528a475fbd8'
# Detach files from vector store
for vs_file in vs_files.data:
    client.vector_stores.files.delete(
        vector_store_id=vectorstore_id,
        file_id=vs_file.id
    )
    print(f"Detached file {vs_file.id} from vector store")

"""## Upload files to file store"""

file_attributes = []
file_metadata = {}
batch_size = 10

metadata = pd.read_excel(METADATA_INDEX_FILE).set_index('converted_file')
files_to_upload = metadata[
    (((metadata['conversion_status'] == 'converted') |
    (metadata['conversion_status'] == 'original')) &
    (metadata['fs_upload_status'] != 'completed'))
]
print(f'{len(files_to_upload)} to upload')

counter = 0
for index, row in files_to_upload.iterrows():
  if row['fs_upload_status'] == 'completed':
    continue
  else:
    try:
        file_id = upload_file_to_openai(index)
        attributes = {
            "converted_file": index,
            "file name": os.path.basename(index),
            "file_id": file_id,
            "uploaded_at": datetime.datetime.now().isoformat(),
            "fs_upload_status": "completed"
        }
        print(f'uploaded file: {index}')
        print(attributes)
        attributes_df = pd.DataFrame([attributes]).set_index('converted_file')
        metadata.update(attributes_df)
        counter += 1
        if counter % batch_size == 0:
          metadata.reset_index().to_excel(METADATA_INDEX_FILE, index=False)
    except ValueError as e:
        print(f"could not upload {index}, str(e)")
metadata.reset_index().to_excel(METADATA_INDEX_FILE, index=False)

"""### Create new vector store, if needed"""

vector_store = client.vector_stores.create(
    name="primesquare",
    file_ids = []
)

"""### Get list of existing vector stores"""

vector_stores = client.vector_stores.list().data
for vs in vector_stores:
  print(f'{vs.id}: {vs.name}: {vs.file_counts.total}')

"""### Attach files to vector store"""

file_metadata = pd.read_excel(METADATA_INDEX_FILE)
# file_metadata = file_metadata.set_index('converted_file').to_dict(orient='records')

file_metadata

# vector_store_id = 'vs_689b49252a4c8191a12a1528a475fbd8' # Replace with your vector store ID

# for file in os.listdir(upload_directory):
#   # file_id = file_metadata[os.path.join(upload_directory, file)]['file_id']
#   file_id = file_metadata[file]['file_id']
#   # file_ = os.path.join(upload_directory, file)
#   print(f'Creating embeddings for {file_id}, {file}')

#   try:
#     client.vector_stores.files.create(
#       vector_store_id = vector_store_id,
#       file_id=file_id,
#       attributes={"filename": file, "file_id": file_id, "category": "PAD file"}
#     )
#   except Exception as e:
#     print(f'{e}\nError creating embeddings for {file_id}, {file}')

for ekg in EKG_map.keys():
  print(f'Creating embeddings for {ekg}')
  vs = EKG_map[ekg]['vector_store']
  files = file_metadata[file_metadata['EKG'] == ekg]

  for index, row in files[:10].iterrows():
    file_name = row['converted_file']
    file = row['file_id']
    print(f'Creating embeddings for {file}')

    try:
      client.vector_stores.files.create_and_poll(
        vector_store_id = vs,
        file_id=file,
        attributes={"filename": file_name, "file_id": file, "EKG": ekg}
      )
    except Exception as e:
      print(f'{e}\nError creating embeddings for {file_id}, {file}')

len(client.vector_stores.files.list('vs_68ea1f9e59b8819193d3c092779bb47e').data)

vectorstore_id = 'vs_68ea1f9e59b8819193d3c092779bb47e'
has_more = True
page = 0

while has_more:
    files = client.vector_stores.files.list(vector_store_id=vectorstore_id)
    for file_ in files:
        print(file_.attributes['filename'])
    # has_more = files.meta['has_next_page']  # Or equivalent flag based on your SDK
    # page += 1

"""# Query on vector store"""

EKG_map['Primesquare']['vector_store']

import os
import openai
from openai import OpenAI

# Set the OPENAI_API_KEY environment variable
os.environ['OPENAI_API_KEY'] = ""
client = OpenAI(api_key = os.getenv("OPENAI_API_KEY"))

# vector_store_ids = [EKG_map['Primesquare']['vector_store']]
vector_store_ids = ['vs_689b49252a4c8191a12a1528a475fbd8']

qlist = pd.read_excel(os.path.join(base_path, 'AI Responses.xlsx'))
qlist['GPT-5'] = ''
questions = qlist['Question'][:5]
# query = "how can I create a project?"
questions = ["""I want to create a quiz comprising of 100 questions.
        Purpose of the quiz is to assess if the business analyst has acquired deep understanding of the product.
        All questions should meet the following criteria -
        1. ONLY the informations embedded in the vectorstore id - 'vs_689b49252a4c8191a12a1528a475fbd8' should be used.
        2. Ignore the knowledge graph for this purpose.
        3. Select questions related to ONLY order management.
        4. Questions should test conceptual understanding as well as knowledge of product details.
        5. About 60% of the questions should test conceptual understanding and 40% should test knowledge of product details.
        6. Avoid trivial questions which are obvious in nature such 'which option to click for creating a new item?' since Add would be an obvious choice.
        7. Avoid question which might be more relevant for engineers rather than Business analysts.
        8. Assign a difficulty level (1-5, where 1 is simplest to answer) to the question and show it along with the answer.
        9. Each question should have 4 choices, one of which would be correct and other will not be correct.
        The right answer should be shown beloweach question.
        Before finalising the questions, evaluate each of them against the criteria 1-9.
        Questions and answers should be suitably worded and easily understandable.
        In case any question does not qualify the suitability, appropriateness or readability test - reformulate the question and options.
        """]

for query in questions:
  print(f'Query: {query}')
  response = client.responses.create(
      model="gpt-5",
      input=query,
      tools=[{
          "type": "file_search",
          "vector_store_ids": vector_store_ids,

          # "max_num_results": 5
      }]
  )

  # qlist.loc[qlist['Question'] == query, 'GPT-5'] = response.output_text
# qlist.to_excel(os.path.join(base_path, 'AI Responses.xlsx'), index=False)

from IPython.display import display, Markdown

display(Markdown(response.output_text))



qlist.to_excel(os.path.join(base_path, 'AI Responses.xlsx'), index=False)
